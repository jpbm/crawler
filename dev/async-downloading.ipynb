{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from concurrent import futures\n",
    "import argparse\n",
    "\n",
    "import requests\n",
    "from bloom_filter import BloomFilter\n",
    "\n",
    "sys.path.append('../data/raw/')\n",
    "\n",
    "urls_seen = BloomFilter(max_elements=500000, error_rate=0.1)\n",
    "urls_to_do = set()\n",
    "\n",
    "def initialize():\n",
    "    response = requests.get(SEED_URL)\n",
    "    global hyperrefs_re\n",
    "    hyperrefs_re = re.compile(SEED_URL+'[a-zA-Z0-9/_-]*[.]html')\n",
    "    global file\n",
    "    file = open(FILENAME,'w')\n",
    "    global file_length\n",
    "    file_length = 0\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        urls = hyperrefs_re.findall(html)\n",
    "        print(\"[initialize] Seeding with %i urls found via %s.\" % (len(set(urls))+1,SEED_URL))\n",
    "        urls_to_do.update(urls)\n",
    "        urls_to_do.update([SEED_URL])\n",
    "    else:\n",
    "        print(\"[initialize] Failed. Code: %i %s\" % (response.status_code,SEED_URL))\n",
    "        \n",
    "def urls_gen():\n",
    "    url = urls_to_do.pop()\n",
    "    return url\n",
    "\n",
    "def process_one(url):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "    url [str]\n",
    "    \n",
    "    From global name space:\n",
    "    file [file object]\n",
    "    hyperrefs_re [regex]\n",
    "    file_length [int]\n",
    "    \"\"\"\n",
    "    \n",
    "    urls_seen.add(url)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        item = \"{'url': %s, 'html': %s}\\n\" % (url,html)\n",
    "        urls_found = hyperrefs_re.findall(html)\n",
    "        for url_found in urls_found:\n",
    "            if not url_found in urls_seen:\n",
    "                urls_to_do.update([url_found])\n",
    "        global file\n",
    "        file.write(item)\n",
    "        global file_length\n",
    "        file_length += 1\n",
    "        print('[process_one] %i processed %i to go.' % (file_length,len(urls_to_do)),end = '\\r')\n",
    "    elif response.status_code == 503:\n",
    "        urls_to_do.update([url])\n",
    "    else:\n",
    "        print('[process_one] Code: %i %s' % (response.status_code,url))\n",
    "\n",
    "\n",
    "def process_many():\n",
    "    \"\"\"\n",
    "    0. Checks if urls_do_do has content\n",
    "    1. Creates copy of urls_to_do\n",
    "    2. Uses threadpool to process all entires in the copy of urls_to_do\n",
    "    3. process one updates the original copy of urls_do_do with new-found urls. Whether a urls is new or not is checked against the bloom filter `urls_seen`.\n",
    "    \n",
    "    Takes:\n",
    "    \n",
    "    From global name space:\n",
    "    urls_to_do\n",
    "    \"\"\"\n",
    "    print('[process_many] Writing to %s' % FILENAME, end = '\\r')\n",
    "    while len(urls_to_do) != 0:\n",
    "        workers = min(MAX_WORKERS,len(urls_to_do))\n",
    "        print('[process_many] Starting loop... multithreading with %i workers' % workers)\n",
    "        with futures.ThreadPoolExecutor(workers) as executor:\n",
    "            list(executor.map(process_one,list(urls_to_do)))\n",
    "    file.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Crawl some website.')\n",
    "    parser.add_argument('seed_url',metavar='u',type=str,help='seed url for the crawler')\n",
    "    parser.add_argument('filename',metavar='d',type=str,help='path and filename')\n",
    "    parser.add_argument('workers',metavar='w',type=str,help='max workers for download',action = 'store_true')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    if args.workers:\n",
    "        MAX_WORKERS = args.workers[0]\n",
    "    else:\n",
    "        MAX_WORKERS = 20\n",
    "    SEED_URL = args.seed_url[0]\n",
    "    FILENAME = args.filename[0]\n",
    "    \n",
    "    print('Crawling %s\\nFilename %s')\n",
    "\n",
    "    initialize()\n",
    "    process_many()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
